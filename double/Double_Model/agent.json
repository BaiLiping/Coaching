{"agent": "ppo", "states": {"type": "float", "shape": [11], "min_value": null, "max_value": null}, "actions": {"type": "float", "shape": [1], "min_value": -1.0, "max_value": 1.0}, "max_episode_timesteps": 1000, "batch_size": 10, "network": {"type": "auto", "depth": 3, "rnn": false}, "use_beta_distribution": true, "memory": "minimum", "update_frequency": 1, "learning_rate": 0.001, "subsampling_fraction": 0.9131375430837279, "optimization_steps": null, "likelihood_ratio_clipping": 0.09955676846552193, "discount": 1.0, "predict_terminal_values": false, "baseline": {"type": "auto", "depth": 3, "rnn": false}, "baseline_optimizer": {"optimizer": "adam", "learning_rate": 0.001, "multi_step": 1}, "state_preprocessing": "linear_normalization", "reward_preprocessing": null, "exploration": {"type": "exponential", "unit": "timesteps", "num_steps": 10, "initial_value": 0.95, "decay_rate": 0.5}, "variable_noise": 0.0, "l2_regularization": 0.0, "entropy_regularization": 0.5, "parallel_interactions": 1, "config": null, "saver": null, "summarizer": null, "recorder": null, "internals": {}, "initial_internals": {"policy": {}, "baseline": {}}}